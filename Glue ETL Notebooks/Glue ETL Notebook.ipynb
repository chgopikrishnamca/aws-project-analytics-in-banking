{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f8312df-32c6-420a-9637-0e900f856684",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETL Notebook"
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Banking Lakehouse POC – Glue + Iceberg (Incremental Load + Alerts)\n",
    "# ==============================================================\n",
    "# This Glue job:\n",
    "# 1. Reads daily CSV files from S3 (Bronze layer)\n",
    "# 2. Transforms and enriches the data\n",
    "# 3. Loads data incrementally into an Iceberg table using MERGE\n",
    "# 4. Sends SNS email alerts on START, SUCCESS, and FAILURE\n",
    "# ==============================================================\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import sys\n",
    "import traceback\n",
    "import boto3\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 0. Global Configuration\n",
    "# --------------------------------------------------------------\n",
    "SNS_TOPIC_ARN = \"arn:aws:sns:eu-north-1:088939728901:bank-app-analytics-topic\"\n",
    "sns = boto3.client(\"sns\")\n",
    "\n",
    "def send_email(subject, message):\n",
    "    sns.publish(\n",
    "        TopicArn=SNS_TOPIC_ARN,\n",
    "        Subject=subject,\n",
    "        Message=message\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 1. Glue & Spark Session Initialization\n",
    "# --------------------------------------------------------------\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 2. Handle Job Parameters (Optional LOAD_DATE)\n",
    "# --------------------------------------------------------------\n",
    "try:\n",
    "    args = getResolvedOptions(sys.argv, [\"JOB_NAME\", \"LOAD_DATE\"])\n",
    "    job_name = args[\"JOB_NAME\"]\n",
    "    load_date = args[\"LOAD_DATE\"]\n",
    "except Exception:\n",
    "    args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "    job_name = args[\"JOB_NAME\"]\n",
    "    load_date = datetime.now(timezone.utc).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "job = Job(glueContext)\n",
    "job.init(job_name, args)\n",
    "\n",
    "start_time = datetime.now(timezone.utc)\n",
    "\n",
    "print(f\"Processing data for date: {load_date}\")\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Send Job START Notification\n",
    "# --------------------------------------------------------------\n",
    "send_email(\n",
    "    subject=f\"Glue Job STARTED: {job_name}\",\n",
    "    message=(\n",
    "        f\"Glue job has started.\\n\\n\"\n",
    "        f\"Job Name : {job_name}\\n\"\n",
    "        f\"Load Date: {load_date}\\n\"\n",
    "        f\"Start Time: {start_time}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "try:\n",
    "    # ----------------------------------------------------------\n",
    "    # 4. Define Input Paths (Bronze Layer)\n",
    "    # ----------------------------------------------------------\n",
    "    # s3://bank-app-data-landing-area/raw/\n",
    "    #   ├── customers/yyyy-mm-dd/\n",
    "    #   ├── accounts/yyyy-mm-dd/\n",
    "    #   └── transactions/yyyy-mm-dd/\n",
    "\n",
    "    base_path = \"s3://bank-app-data-landing-area/raw\"\n",
    "\n",
    "    cust_path = f\"{base_path}/customers/{load_date}/\"\n",
    "    acc_path  = f\"{base_path}/accounts/{load_date}/\"\n",
    "    tx_path   = f\"{base_path}/transactions/{load_date}/\"\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 5. Read Bronze CSV Data\n",
    "    # ----------------------------------------------------------\n",
    "    cust_df = spark.read.option(\"header\", True).csv(cust_path)\n",
    "    acc_df  = spark.read.option(\"header\", True).csv(acc_path)\n",
    "    tx_df   = spark.read.option(\"header\", True).csv(tx_path)\n",
    "\n",
    "    print(\"Bronze data loaded successfully\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 6. Data Transformations\n",
    "    # ----------------------------------------------------------\n",
    "    tx_df = (\n",
    "        tx_df.withColumnRenamed(\"txn_id\", \"transaction_id\")\n",
    "             .withColumn(\"amount\", col(\"amount\").cast(DecimalType(12, 2)))\n",
    "             .withColumn(\"txn_date\", to_date(\"timestamp\"))\n",
    "    )\n",
    "\n",
    "    df_enriched = (\n",
    "        tx_df.join(acc_df, \"account_id\")\n",
    "             .join(cust_df, \"customer_id\")\n",
    "    )\n",
    "\n",
    "    record_count = df_enriched.count()\n",
    "\n",
    "    if record_count == 0:\n",
    "        raise Exception(\"No records found after transformation\")\n",
    "\n",
    "    print(f\"Transformations completed. Records processed: {record_count}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 7. Ensure Iceberg Database & Table Exist\n",
    "    # ----------------------------------------------------------\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS analytics_db\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS analytics_db.enriched_transactions (\n",
    "        transaction_id STRING,\n",
    "        account_id      STRING,\n",
    "        customer_id     STRING,\n",
    "        amount          DECIMAL(12,2),\n",
    "        timestamp       STRING,\n",
    "        txn_date        DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Iceberg database and table verified\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 8. Incremental Load using MERGE INTO\n",
    "    # ----------------------------------------------------------\n",
    "    df_enriched.createOrReplaceTempView(\"tmp_enriched\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    MERGE INTO analytics_db.enriched_transactions t\n",
    "    USING tmp_enriched s\n",
    "    ON t.transaction_id = s.transaction_id\n",
    "\n",
    "    WHEN MATCHED THEN\n",
    "      UPDATE SET\n",
    "        t.account_id = s.account_id,\n",
    "        t.customer_id = s.customer_id,\n",
    "        t.amount = s.amount,\n",
    "        t.timestamp = s.timestamp,\n",
    "        t.txn_date = s.txn_date\n",
    "\n",
    "    WHEN NOT MATCHED THEN\n",
    "      INSERT (\n",
    "        transaction_id,\n",
    "        account_id,\n",
    "        customer_id,\n",
    "        amount,\n",
    "        timestamp,\n",
    "        txn_date\n",
    "      )\n",
    "      VALUES (\n",
    "        s.transaction_id,\n",
    "        s.account_id,\n",
    "        s.customer_id,\n",
    "        s.amount,\n",
    "        s.timestamp,\n",
    "        s.txn_date\n",
    "      )\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"MERGE operation completed\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # 9. SUCCESS Notification\n",
    "    # ----------------------------------------------------------\n",
    "    send_email(\n",
    "        subject=f\"Glue Job SUCCESS: {job_name}\",\n",
    "        message=(\n",
    "            f\"Glue job completed successfully.\\n\\n\"\n",
    "            f\"Job Name : {job_name}\\n\"\n",
    "            f\"Load Date: {load_date}\\n\"\n",
    "            f\"Records Processed: {record_count}\\n\"\n",
    "            f\"End Time: {datetime.now(timezone.utc)}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    job.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    # ----------------------------------------------------------\n",
    "    # 10. FAILURE Notification\n",
    "    # ----------------------------------------------------------\n",
    "    error_message = traceback.format_exc()\n",
    "\n",
    "    send_email(\n",
    "        subject=f\"Glue Job FAILED: {job_name}\",\n",
    "        message=(\n",
    "            f\"Glue job failed.\\n\\n\"\n",
    "            f\"Job Name : {job_name}\\n\"\n",
    "            f\"Load Date: {load_date}\\n\"\n",
    "            f\"Failure Time: {datetime.now(timezone.utc)}\\n\\n\"\n",
    "            f\"Error Details:\\n{error_message}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Job failed\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9487ce1c-c344-4c28-b646-a9830c535dc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Instructions to run the above script:\n",
    "- Create new bucket for iceberg tables.\n",
    "- Create another bucket for input files and place them in respective folders.\n",
    "- Create a new glue script using script editor in AWS Glue.\n",
    "- Copy Paste above script.\n",
    "- Add job parameters (Key, Values) \n",
    "\n",
    " --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO --conf spark.sql.catalog.glue_catalog.warehouse=s3://bank-app-lakehouse-iceberg/warehouse/ --conf spark.sql.defaultCatalog=glue_catalog\n",
    "\n",
    "-- --datalake-formats iceberg\n",
    "\n",
    "- Run the script."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Glue ETL Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
